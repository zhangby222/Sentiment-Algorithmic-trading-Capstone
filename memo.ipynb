{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. EDA"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Count unique values in a vector (while dropping the NaN)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This return a series\n",
    "df['Borough'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescaling the axis using log\n",
    "df['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=True, logy=True)\n",
    "df.plot(kind='scatter', x='initial_cost', y='total_est_fee', rot=70)\n",
    "df.boxplot(column='initial_cost', by='Borough', rot=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Data Cleaning : Melt, pivot, pivottable"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.1 Transform into panel data: pd.melt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df =        \n",
    "#         Date  Ozone  Solar.R  Wind  Temp\n",
    "# 0 1973-05-01   41.0    190.0   7.4    67\n",
    "# 1 1973-05-02   36.0    118.0   8.0    72\n",
    "# 2 1973-05-03   12.0    149.0  12.6    74\n",
    "# 3 1973-05-04   18.0    313.0  11.5    62\n",
    "# 4 1973-05-05    NaN      NaN  14.3    56\n",
    "\n",
    "# Melt them all\n",
    "pd.melt(frame=df, id_vars=['Date'])\n",
    "\n",
    "#           Date variable  value\n",
    "# 0   1973-05-01    Ozone   41.0\n",
    "# 1   1973-05-02    Ozone   36.0\n",
    "# 2   1973-05-03    Ozone   12.0\n",
    "# 3   1973-05-04    Ozone   18.0\n",
    "# 4   1973-05-05    Ozone    NaN\n",
    "# 5   1973-05-06    Solar.R   28.0\n",
    "# 6   1973-05-07    Solar.R   23.0\n",
    "\n",
    "# Only keep the value of selected columns and name the new columns\n",
    "pd.melt(frame=df, id_vars=['Date'], value_vars=['Temp', 'Wind'], var_name='Temp and Wind', value_name='Quantity')\n",
    "\n",
    "#     Date Temp and Wind  Quantity\n",
    "# 0     NaN          Temp      67.0\n",
    "# 1     NaN          Temp      72.0\n",
    "# 2     NaN          Temp      74.0\n",
    "# 3     NaN          Temp      62.0\n",
    "# 4     NaN          Temp      56.0\n",
    "# 5     NaN          Temp      66.0\n",
    "# 6     NaN          Temp      65.0b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â€¢ Transform back: pd.pivot"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot(data=df, index='Date', columns='variable', values='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.2 Only Transform back part of columns and use function to the rest columns: pd.pivot_table\n",
    "- Appliable when we only care about some of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df\n",
    "#      A    B      C  D  E\n",
    "# 0  foo  one  small  1  2\n",
    "# 1  foo  one  large  2  4\n",
    "# 2  foo  one  large  2  5\n",
    "# 3  foo  two  small  3  5\n",
    "# 4  foo  two  small  3  6\n",
    "# 5  bar  one  large  4  6\n",
    "# 6  bar  one  small  5  8\n",
    "# 7  bar  two  small  6  9\n",
    "# 8  bar  two  large  7  9\n",
    "\n",
    "# In this example, we take the mean of bar-large-one and bar-large-tow and we get 5.50\n",
    "# we use dictionary to manipulate different columns\n",
    "\n",
    "table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n",
    "                    aggfunc={'D': np.mean,\n",
    "                             'E': [min, max, np.mean]})\n",
    "# table\n",
    "#                 D    E\n",
    "#             mean  max      mean  min\n",
    "# A   C\n",
    "# bar large  5.500000  9.0  7.500000  6.0\n",
    "#     small  5.500000  9.0  8.500000  8.0\n",
    "# foo large  2.000000  5.0  4.500000  4.0\n",
    "#     small  2.333333  6.0  4.333333  2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Data cleaning: Splitting columns using .str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.1 Slicing the column name directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#      country  year variable   value\n",
    "# 0         AD  2000     m014     0.0\n",
    "# 1         AE  2000     m014     2.0\n",
    "# 2         AF  2000     m014    52.0\n",
    "# 3         AG  2000     m014     0.0\n",
    "# 4         AL  2000     m014     2.0\n",
    "\n",
    "# Create the 'gender' column\n",
    "tb_melt['gender'] = tb_melt.variable.str[0]b\n",
    "\n",
    "# Create the 'age_group' column\n",
    "tb_melt['age_group'] = tb_melt.variable.str[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.2 Use certain pattern to split them and get splitted parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'str_split' column\n",
    "ebola_melt['str_split'] = ebola_melt['type_country'].str.split('_')\n",
    "\n",
    "# In [4]: ebola_melt.head()\n",
    "# Out[4]: \n",
    "#          Date  Day  type_country  counts        str_split\n",
    "# 0    1/5/2015  289  Cases_Guinea  2776.0  [Cases, Guinea]\n",
    "# 1    1/4/2015  288  Cases_Guinea  2775.0  [Cases, Guinea]\n",
    "# 2    1/3/2015  287  Cases_Guinea  2769.0  [Cases, Guinea]\n",
    "# 3    1/2/2015  286  Cases_Guinea     NaN  [Cases, Guinea]\n",
    "# 4  12/31/2014  284  Cases_Guinea  2730.0  [Cases, Guinea]\n",
    "\n",
    "# Create the 'type' column\n",
    "ebola_melt['type'] = ebola_melt['str_split'].str.get(0)\n",
    "\n",
    "# Create the 'country' column\n",
    "ebola_melt['country'] = ebola_melt['str_split'].str.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.3 Use expand=True directly but need to rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebola_melt = ebola_melt.drop(columns = ['type_country']).concat(ebola_melt['type_country'].str.split('_', expand=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Data cleaning: Concat, Merge and Join data\n",
    "\n",
    "    - Concat gives the flexibility to join based on the axis( all rows or all columns)\n",
    "    - Append is the specific case(axis=0, join='outer') of concat\n",
    "    - Join is based on the indexes (set by set_index) on how variable =\\['left','right','inner','couter'\\]\n",
    "    - Merge is based on any particular column each of the two dataframes, this columns are variables on like 'left_on', 'right_on', 'on'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default is to concat dfs row-wise. they should have the same columns\n",
    "row_concat = pd.concat([uber1, uber2, uber3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dfs column-wise. they should have the same number of rows.\n",
    "ebola_tidy = pd.concat([ebola_melt, status_country], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2m = pd.merge(left=m2m, right=survey, left_on='ident', right_on='taken')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Data cleaning: using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.5 Check if the pattern is matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True\n"
    }
   ],
   "source": [
    "''' Compile pattern then match'''\n",
    "\n",
    "# Import the regular expression module\n",
    "import re\n",
    "\n",
    "# Compile the pattern: prog\n",
    "prog = re.compile('\\d{3}-\\d{3}-\\d{3}')\n",
    "\n",
    "# See if the pattern matches\n",
    "result = prog.match('123-456-7890')\n",
    "print(bool(result))\n",
    "\n",
    "''' Match directly'''\n",
    "pattern1 = bool(re.match(pattern='\\d{3}-\\d{3}-\\d{4}', string='123-456-7890'))\n",
    "pattern2 = bool(re.match('\\$\\d*\\.\\d{2}', string = '$123.45'))\n",
    "pattern3 = bool(re.match('[A-Z]\\w*', string = 'Australia'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.6 Modify columns using certain pattern - use re.findall and .replace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = re.findall('\\d+', 'the recipe calls for 10 strawberries and 1 banana')\n",
    "# for dfs The data is like $16.99 \n",
    "tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\\d+\\.\\d+', x)[0])\n",
    "# use replace to modify values\n",
    "tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Deal with missing values and dupilcate values"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Drop duplicate values: df.drop_duplicates()'''\n",
    "tracks_no_duplicates = tracks.drop_duplicates()\n",
    "\n",
    "'''Drop NA''' \n",
    "df.dropna(how = 'any', axix=1) # if any missing value, drop column\n",
    "df.dropna(how = 'all', axis=0) # if all missing value, drop row\n",
    "\n",
    "# check the sum of missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "'''Replace NaN with something'''\n",
    "oz_mean = airquality.Ozone.mean()\n",
    "\n",
    "# Replace all the missing values in the Ozone column with the mean\n",
    "airquality['Ozone'] = airquality.Ozone.fillna(airquality.Ozone.mean())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Data manipulation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 dict manipulation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.1.1 Extract / Modify indexes"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Dell\\\\Desktop\\\\MachineLearning\\\\Notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europe = {'spain':'madrid', 'france':'paris', 'germany':'berlin', 'norway':'oslo' }\n",
    "\n",
    "# Add italy to europe\n",
    "europe['italy']='rome'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Dataframe Manipulation\n",
    "\n",
    "don't forget .reset_index() everytime after merging them"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.1 Converting everything into dataframe"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy array : turnout_zscore is an n*1 array\n",
    "election['turnout_zscore'] = turnout_zscore # OR\n",
    "pd.Dataframe(turnout_zscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.2 Change data types\n",
    "\n",
    "    You can use this to manipulating datetime\n",
    "    You can also use it to create dummy varibles for columns that already have binary values\n",
    "    Use error=coerce whenever necessary"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime and numeric type \n",
    "# Corerce in case there are some dirty values\n",
    "News.date = pd.to_datetime(News.date)\n",
    "pd.to_numeric(News.year, errors='coerce')\n",
    "ser.astype('int64')\n",
    "\n",
    "# extract years from datetime\n",
    "df['year'] = pd.DatetimeIndex(df['birth_date']).year\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.3 extract / modify indexes"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_labels = ['US', 'AUS', 'JPN', 'IN', 'RU', 'MOR', 'EG']\n",
    "\n",
    "cars.index = row_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.4 Read CSV"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('cars.csv', index_col=0)\n",
    "cars = pd.read_csv('cars.csv', index_col='serial number', parse_dates='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.5 Read multiple files with a similar pattern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all csv files\n",
    "\n",
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Write the pattern: pattern.  * is a wild card\n",
    "pattern = '*.csv'\n",
    "\n",
    "# Save all file matches: csv_files\n",
    "'''csv_files is a list full of file names'''\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# Load the second file into a DataFrame: csv2\n",
    "csv1 = pd.read_csv(csv_files[0])\n",
    "\n",
    "# Load all files and put them into a list\n",
    "frames = []\n",
    "for csv in csv_files:\n",
    "    df = pd.read_csv(csv)\n",
    "    frames.append(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.5.1 Read all csv files into a list and concat them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list: frames\n",
    "frames = []\n",
    "\n",
    "#  Iterate over csv_files\n",
    "for csv in csv_files:\n",
    "\n",
    "    #  Read csv into a DataFrame: df\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # Append df to frames\n",
    "    frames.append(df)\n",
    "\n",
    "# Concatenate frames into a single DataFrame: uber\n",
    "uber = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.5.2 A home-made csv-reading into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_dict = {}\n",
    "news_dict = {}\n",
    "for i in range(1, 16):\n",
    "    if i < 10:\n",
    "        fx_dict['20190{}'.format(i)] = pd.read_csv('fx_20190{}.csv'.format(i), parse_dates=['time'])\n",
    "        news_dict['20190{}'.format(i)] = pd.read_csv('News_20190{}.csv'.format(i), parse_dates=['date'])\n",
    "    elif i < 13: \n",
    "        fx_dict['2019{}'.format(i)] = pd.read_csv('fx_2019{}.csv'.format(i), parse_dates=['time'])\n",
    "        news_dict['2019{}'.format(i)] = pd.read_csv('News_2019{}.csv'.format(i), parse_dates=['date'])\n",
    "    else: \n",
    "        fx_dict['20200{}'.format(i-12)] = pd.read_csv('fx_20200{}.csv'.format(i-12), parse_dates=['time'])\n",
    "        news_dict['20200{}'.format(i-12)] = pd.read_csv('News_20200{}.csv'.format(i-12), parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.5.3 Export csv files all altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 16):\n",
    "    if i < 10:\n",
    "        fx[(fx.time.dt.year == 2019) & (fx.time.dt.month == i)].to_csv('fx_20190{}.csv'.format(i))\n",
    "        News[(News.date.dt.year == 2019) & (News.date.dt.month == i)].to_csv('News_20190{}.csv'.format(i))\n",
    "    elif i < 13:\n",
    "        fx[(fx.time.dt.year == 2019) & (fx.time.dt.month == i)].to_csv('fx_2019{}.csv'.format(i))\n",
    "        News[(News.date.dt.year == 2019) & (News.date.dt.month == i)].to_csv('News_2019{}.csv'.format(i))\n",
    "    else:\n",
    "        fx[(fx.time.dt.year == 2020) & (fx.time.dt.month == i-12)].to_csv('fx_20200{}.csv'.format(i-12))\n",
    "        News[(News.date.dt.year == 2020) & (News.date.dt.month == i-12)].to_csv('News_20200{}.csv'.format(i-12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract elements / slicing pieces from df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract: double [] returns df, single[] returns series\n",
    "cars.loc[:,['country','drives_right']]\n",
    "cars.loc[['RU', 'CN'],['country','drives_right']]\n",
    "cars.iloc[[1, 2, 3],['country','drives_right']]\n",
    "cars.iloc[[1, 2, 3],[1, 2, 3]]\n",
    "\n",
    "# slicing: single[] returns df.\n",
    "p_counties = election.loc['Perry': 'Potter', :]\n",
    "p_counties = election.loc[: 'Potter', :]\n",
    "p_counties_rev = election.loc['Potter': 'Perry': -1] # slice and arrange them reversely\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slicing with condition - including Non-zero or Non-null condition"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing with condition: single[]\n",
    "high_turnout = election.turnout > 70\n",
    "high_turnout_df = election[high_turnout]\n",
    "\n",
    "# Multiple conditions\n",
    "df[(df.salt >= 50) & (df.eggs < 200)]\n",
    "\n",
    "df2.loc[:, df2.all()] # keep columns with all non-zero values\n",
    "df2.loc[:, df2.any()] # keep columns with not-all-zero values\n",
    "df.loc[:, df.isnull().any()] # keep columns with any NaN values\n",
    "df.loc[:, df.notnull().all()] # keep columns with no NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create columns using loc"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change all elements in a column - including creating dummies\n",
    "    - Using pd.series.apply() or .map()\n",
    "    - Using vectorized functions if the we want shorter running time - at the same speed with compiled code(C)\n",
    "    - If the column already has clean binary values (like 'Male' and 'Female'), use .astype('category') directly."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without condition\n",
    "\n",
    "for lab, row in cars.iterrows():\n",
    "    cars.loc[lab, \"COUNTRY\"] = row[\"country\"].upper()\n",
    "#\n",
    "cars[\"COUNTRY\"] = cars[\"country\"].apply(str.upper)\n",
    "\n",
    "# Create dummy in the dummiest way\n",
    "\n",
    "def recode_gender(gender):\n",
    "    if gender == 'Female':\n",
    "        return 0\n",
    "    elif gender == 'Male':\n",
    "        return 1 \n",
    "    else:\n",
    "        return np.nan\n",
    "tips['recode'] = tips.sex.apply(recode_gender)\n",
    "\n",
    "# with condition: perfect for creating dummy or categorical variables - using map\n",
    "red_vs_blue = {'Obama':'blue', 'Romney':\"red\"}\n",
    "election['color'] = election.winner.map(red_vs_blue)\n",
    "\n",
    "# Use np.where - faster\n",
    "\n",
    "merged['if_16_NaN'] = np.where(merged.specialty_description_16.isnull(), 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# if else & numpy operator"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if room == \"kit\" :\n",
    "    print(\"looking around in the kitchen.\")\n",
    "elif room == \"bed\":\n",
    "    print(\"looking around in the bedroom.\")\n",
    "else :\n",
    "    print(\"looking around elsewhere.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi = np.array([ 21.852, 20.975, 21.75 , 24.747, 21.441]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bmi > 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 list manipulation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concatenate and slicing"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Concatenate shout1 with shout2: new_shout\n",
    "    new_shout = shout1 + shout2\n",
    "    # extract elements\n",
    "    even_nums = (2,nums[1],nums[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply a function to every element in a list - use map - can use list comprehension"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of strings: spells\n",
    "spells = [\"protego\", \"accio\", \"expecto patronum\", \"legilimens\"]\n",
    "\n",
    "# Use map() to apply a lambda function over spells: shout_spells\n",
    "shout_spells = map(lambda item: item + '!!!', spells)\n",
    "\n",
    "# Convert shout_spells to a list: shout_spells_list\n",
    "shout_spells_list = list(shout_spells)\n",
    "\n",
    "# Print the result\n",
    "print(shout_spells_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter every element in a list \n",
    "    - can use list comprehension instead\n",
    "    - can be used to pandas series as well"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use list comprehension instead\n",
    "new_fellowship = [member for member in fellowship if len(member) >= 7]\n",
    "\n",
    "# Create a list of strings: fellowship\n",
    "fellowship = ['frodo', 'samwise', 'merry', 'pippin', 'aragorn', 'boromir', 'legolas', 'gimli', 'gandalf']\n",
    "\n",
    "# Use filter() to apply a lambda function over fellowship: result\n",
    "result = filter(lambda member: len(member) > 6, fellowship)\n",
    "\n",
    "# Convert result to a list: result_list\n",
    "result_list = list(result)\n",
    "\n",
    "# Print result_list\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add up every element of the list- reduce"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of strings: fellowship\n",
    "fellowship = ['frodo', 'samwise', 'merry', 'pippin', 'aragorn', 'boromir', 'legolas', 'gimli', 'gandalf']\n",
    "\n",
    "# Use filter() to apply a lambda function over fellowship: result\n",
    "result = filter(lambda member: len(member) > 6, fellowship)\n",
    "\n",
    "# Convert result to a list: result_list\n",
    "result_list = list(result)\n",
    "\n",
    "# Print result_list\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count the items in one df and put them in to dict"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define count_entries()\n",
    "def count_entries(df, col_name):\n",
    "    \"\"\"Return a dictionary with counts of \n",
    "    occurrences as value for each key.\"\"\"\n",
    "\n",
    "    # Initialize an empty dictionary: langs_count\n",
    "    langs_count = {}\n",
    "    \n",
    "    # Extract column from DataFrame: col\n",
    "    col = df[col_name]\n",
    "    \n",
    "    # Iterate over lang column in DataFrame\n",
    "    for entry in col:\n",
    "\n",
    "        # If the language is in langs_count, add 1\n",
    "        if entry in langs_count.keys():\n",
    "            langs_count[entry] += 1\n",
    "        # Else add the language to langs_count, set the value to 1\n",
    "        else:\n",
    "            langs_count[entry] = 1\n",
    "\n",
    "    # Return the langs_count dictionary\n",
    "    return langs_count\n",
    "\n",
    "# Call count_entries(): result\n",
    "result = count_entries(tweets_df, 'lang')\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### while loop"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while offset != 0 :\n",
    "    print(\"correcting...\")\n",
    "    offset = offset - 1\n",
    "    print(offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for loop"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for loop with enumerate"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = [11.25, 18.0, 20.0, 10.75, 9.50]\n",
    "for index, area in enumerate(areas) :\n",
    "    index = index + 1\n",
    "    print(\"room \" + str(index) + \": \" + str(area))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop through dicts"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of dictionary\n",
    "europe = {'spain':'madrid', 'france':'paris', 'germany':'berlin',\n",
    "          'norway':'oslo', 'italy':'rome', 'poland':'warsaw', 'austria':'vienna' }\n",
    "          \n",
    "# Iterate over europe\n",
    "for key, value in europe.items():\n",
    "    print(\"the capital of \"+ str(key)+\" is \"+ str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop over numpy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop over np_height\n",
    "for x in np_height:\n",
    "    print(str(x) + \" inches\")\n",
    "\n",
    "# For loop over every element in 2d array np_baseball\n",
    "for x in np.nditer(np_baseball):\n",
    "    print(str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over rows of cars\n",
    "# This prints every index name and all contents in that row.\n",
    "for lab, row in cars.iterrows():\n",
    "    print(lab)\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rand"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# Generate and print random float\n",
    "print(np.random.rand())\n",
    "print(np.random.randint(1,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rand walk"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and matplotlib imported, seed set\n",
    "\n",
    "# Simulate random walk 250 times\n",
    "all_walks = []\n",
    "for i in range(10) :\n",
    "    random_walk = [0]\n",
    "    for x in range(100) :\n",
    "        step = random_walk[-1]\n",
    "        dice = np.random.randint(1,7)\n",
    "        if dice <= 2:\n",
    "            step = max(0, step - 1)\n",
    "        elif dice <= 5:\n",
    "            step = step + 1\n",
    "        else:\n",
    "            step = step + np.random.randint(1,7)\n",
    "\n",
    "        # Implement clumsiness\n",
    "        if ___ :\n",
    "            step = 0\n",
    "\n",
    "        random_walk.append(step)\n",
    "    all_walks.append(random_walk)\n",
    "\n",
    "# Create and plot np_aw_t\n",
    "np_aw_t = np.transpose(np.array(all_walks))\n",
    "plt.plot(np_aw_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try-except error handling- datacamp toolbox 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other notes"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Why use set for stopwords?\n",
    "'''\n",
    "# Lists are slightly faster than sets when you just want to iterate over the values. Sets, however, are significantly faster than lists if you want to check if an item is contained within it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escape special characters when explicitly using regular expressions.\n",
    "    use / before the re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([\"1+1=2\"])\n",
    "s.str.split(r\"\\+|=\", expand=True)\n",
    "     0    1    2\n",
    "0    1    1    2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the value is true, then it will not return anything. If False, it returns an AssertionError\n",
    "assert ebola.notnull().all().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeit"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "input_list = range(100)\n",
    "# def div_by_five(num):\n",
    "#     if num % 5 == 0:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "# xyz = (i for i in input_list if div_by_five(i))\n",
    "\n",
    "\n",
    "print(timeit.timeit('''def div_by_five(num):\n",
    "    if num % 5 == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "xyz = (i for i in input_list if div_by_five(i))\n",
    "''', number = 5000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful shortcuts - vscode"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctrl + k + c : turn snippet into comments\n",
    "# ctrl + k + u : turn comments back into codes"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda3fb0f09c161d4d90ae4912942e5fcef4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}